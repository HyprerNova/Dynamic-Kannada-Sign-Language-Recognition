{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install mediapipe opencv-python tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout,\n",
    "    LSTM, TimeDistributed, Bidirectional,\n",
    "    Conv1D, MaxPooling1D, SeparableConv1D, SeparableConv2D,\n",
    "    Activation, Masking, Input, LayerNormalization\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"/content/drive/MyDrive/Merged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Parameters\n",
    "# (x,y,z,visibility) -> for pose\n",
    "# (x,y,z) -> for hands\n",
    "MAX_SEQUENCE_LENGTH = 75\n",
    "NUM_FEATURES = 162\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "CLASSES = ['Afternoon', 'Apple', 'April', 'August', 'Banana', 'Day', 'December', 'Evening',\n",
    "           'Febraury', 'Friday', 'Grapes', 'January', 'July', 'June', 'March', 'May', 'Monday',\n",
    "           'Morning', 'Night', 'November', 'October', 'Orange', 'Rainy', 'Saturday', 'September',\n",
    "           'Summer', 'Sunday', 'Thursday', 'Tuesday', 'Valencia_Orange', 'Watermelon', 'Wednesday', 'Winter']\n",
    "\n",
    "NUM = len(CLASSES)\n",
    "\n",
    "print(NUM)\n",
    "print(NUM_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def extract_mediapipe_features(video_path, display_video=False):\n",
    "    sequence_features = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        return None\n",
    "\n",
    "    desired_pose_landmark_indices = set(range(11,23))\n",
    "\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        frame_count = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Convert the BGR image to RGB.\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False # To improve performance\n",
    "\n",
    "            # Process the image and find landmarks.\n",
    "            results = holistic.process(image)\n",
    "\n",
    "            # Revert to BGR and enable writing for drawing\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # --- Extract only desired features ---\n",
    "            current_frame_features = []\n",
    "\n",
    "            # Pose landmarks (only desired upper body points, including the first 11)\n",
    "            if results.pose_landmarks:\n",
    "                for i, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "                    if i in desired_pose_landmark_indices:\n",
    "                        current_frame_features.extend([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "                # Pad if not all desired pose landmarks were detected or if some are skipped\n",
    "                num_expected_pose_features = len(desired_pose_landmark_indices) * 3\n",
    "                if len(current_frame_features) < num_expected_pose_features:\n",
    "                    # Pad with zeros for any missing desired landmarks\n",
    "                    current_frame_features.extend([0.0] * (num_expected_pose_features - len(current_frame_features)))\n",
    "            else:\n",
    "                # If no pose landmarks were detected at all, fill with zeros for all expected pose features\n",
    "                current_frame_features.extend([0.0] * len(desired_pose_landmark_indices) * 3)\n",
    "\n",
    "            # Left hand landmarks (21 landmarks, each with x, y, z)\n",
    "            if results.left_hand_landmarks:\n",
    "                for landmark in results.left_hand_landmarks.landmark:\n",
    "                    current_frame_features.extend([landmark.x, landmark.y, landmark.z])\n",
    "            else:\n",
    "                current_frame_features.extend([0.0] * 21 * 3) # 63 zeros\n",
    "\n",
    "            # Right hand landmarks (21 landmarks, each with x, y, z)\n",
    "            if results.right_hand_landmarks:\n",
    "                for landmark in results.right_hand_landmarks.landmark:\n",
    "                    current_frame_features.extend([landmark.x, landmark.y, landmark.z])\n",
    "            else:\n",
    "                current_frame_features.extend([0.0] * 21 * 3) # 63 zeros\n",
    "\n",
    "            # Verify the total number of features matches NUM_FEATURES\n",
    "            if len(current_frame_features) != NUM_FEATURES:\n",
    "                print(f\"Warning: Feature count mismatch at frame {frame_count}. Expected {NUM_FEATURES}, got {len(current_frame_features)}\")\n",
    "                # Fallback to ensure consistent shape by padding with zeros if mismatch occurs\n",
    "                if len(current_frame_features) < NUM_FEATURES:\n",
    "                     current_frame_features.extend([0.0] * (NUM_FEATURES - len(current_frame_features)))\n",
    "                else: # Truncate if too many (unlikely with this logic but good practice)\n",
    "                    current_frame_features = current_frame_features[:NUM_FEATURES]\n",
    "\n",
    "\n",
    "            sequence_features.append(current_frame_features)\n",
    "            frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    if display_video:\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    if not sequence_features:\n",
    "        print(f\"Warning: No features extracted from {video_path}\")\n",
    "        return np.zeros((0, NUM_FEATURES)) # Return empty array with correct feature dimension\n",
    "\n",
    "    return np.array(sequence_features)\n",
    "\n",
    "\n",
    "def standardize_sequence(sequence, max_len, num_features):\n",
    "    \"\"\"\n",
    "    Pads, truncates, or extracts the starting segment of a sequence to a fixed length.\n",
    "    \"\"\"\n",
    "    current_len = len(sequence)\n",
    "\n",
    "    if current_len > max_len:\n",
    "        # Take the starting max_len elements\n",
    "        return sequence[:max_len]\n",
    "    elif current_len < max_len:\n",
    "        # Pad the sequence with zeros\n",
    "        padding = np.zeros((max_len - current_len, num_features))\n",
    "        return np.vstack((sequence, padding))\n",
    "    else:\n",
    "        # Sequence is already the correct length\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. Load Data (Video Paths and Labels) ---\n",
    "def load_data(data_path, classes_list, max_seq_length, num_features_per_frame):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    label_map = {label: num for num, label in enumerate(classes_list)}\n",
    "\n",
    "    for class_name in classes_list:\n",
    "        class_path = os.path.join(data_path, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            print(f\"Warning: Directory not found for class {class_name} at {class_path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing class: {class_name}\")\n",
    "        video_count = 0\n",
    "        for video_file in os.listdir(class_path):\n",
    "            video_path = os.path.join(class_path, video_file)\n",
    "            # Basic check for video file extensions, add more if needed\n",
    "            if not (video_file.lower().endswith('.mp4') or \\\n",
    "                    video_file.lower().endswith('.avi') or \\\n",
    "                    video_file.lower().endswith('.mov')):\n",
    "                print(f\"Skipping non-video file: {video_file} in {class_name}\")\n",
    "                continue\n",
    "\n",
    "            # Set display_video=True for debugging a single video, False for batch processing\n",
    "            keypoints = extract_mediapipe_features(video_path, display_video=False)\n",
    "\n",
    "            if keypoints is not None and keypoints.shape[0] > 0:\n",
    "                # Preprocess: pad or truncate\n",
    "                processed_keypoints = standardize_sequence(keypoints, max_len=max_seq_length, num_features=num_features_per_frame)\n",
    "                sequences.append(processed_keypoints)\n",
    "                labels.append(label_map[class_name])\n",
    "                video_count +=1\n",
    "            else:\n",
    "                print(f\"Warning: Could not extract features or no frames from {video_path}. Skipping.\")\n",
    "        print(f\"Processed {video_count} videos for class {class_name}\")\n",
    "\n",
    "\n",
    "    if not sequences:\n",
    "        print(\"Error: No sequences were loaded. Check DATA_PATH and video files.\")\n",
    "        return None, None\n",
    "\n",
    "    return np.array(sequences), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X, y = load_data(DATA_PATH, CLASSES, MAX_SEQUENCE_LENGTH, NUM_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# prompt: train test and validate the data in the ratio = 7:2:1\n",
    "\n",
    "# Calculate the sizes for train, validation, and test sets\n",
    "train_size = 0.7\n",
    "val_size = 0.1\n",
    "test_size = 0.2\n",
    "\n",
    "# Split the data into training and remaining (validation + test)\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=train_size, random_state=42, stratify=y)\n",
    "\n",
    "# Calculate the ratio of validation and test sets from the remaining data\n",
    "# val_size_rem = val_size / (val_size + test_size)\n",
    "# test_size_rem = test_size / (val_size + test_size) # This is 1 - val_size_rem\n",
    "\n",
    "# Split the remaining data into validation and test sets\n",
    "# Since we want a 7:2:1 split of the *original* data, the ratio of val:test\n",
    "# within the remaining data (y_rem) is val_size / (val_size + test_size)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, test_size=(test_size/(val_size + test_size)), random_state=42, stratify=y_rem)\n",
    "\n",
    "\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_val: {X_val.shape}\")\n",
    "print(f\"Shape of y_val: {y_val.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- One-Hot Encode Labels ---\n",
    "y_train_cat = to_categorical(y_train, num_classes=NUM)\n",
    "y_val_cat = to_categorical(y_val, num_classes=NUM)\n",
    "y_test_categorical = to_categorical(y_test, num_classes=NUM) # Rename to y_test_categorical to avoid conflict later\n",
    "\n",
    "\n",
    "print(f\"Shape of y_train after one-hot encoding: {y_train_cat.shape}\")\n",
    "print(f\"Shape of y_val after one-hot encoding: {y_val_cat.shape}\")\n",
    "print(f\"Shape of y_test after one-hot encoding: {y_test_categorical.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_stacked_bilstm_model(input_shape, num_classes, lstm_units=512, dropout_rate=0.6):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    masked_input = Masking(mask_value=0.)(inputs)\n",
    "\n",
    "    # First BiLSTM layer with increased units\n",
    "    x = Bidirectional(LSTM(lstm_units, return_sequences=True))(masked_input)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Second BiLSTM layer\n",
    "    x = Bidirectional(LSTM(lstm_units, return_sequences=False))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Dense layers with L2 regularization\n",
    "    x = Dense(lstm_units, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = RMSprop(learning_rate=0.0005)  # Adjusted learning rate\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Add early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "    return model, early_stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "input_shape = (MAX_SEQUENCE_LENGTH, NUM_FEATURES)\n",
    "model, early_stopping = create_stacked_bilstm_model(input_shape, NUM) # Unpack the tuple\n",
    "model.summary()\n",
    "\n",
    "# Redundant callback definitions removed\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001, verbose=1)\n",
    "callbacks_list = [early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# prompt: fit the model\n",
    "if X_train.shape[0] > 0 :\n",
    "    print(\"\\n--- Starting Model Training ---\")\n",
    "    model_obj, _ = model \n",
    "\n",
    "    history = model_obj.fit( # Use the unpacked model object here\n",
    "      X_train, y_train_cat,\n",
    "      epochs=200,\n",
    "      batch_size=BATCH_SIZE,\n",
    "      validation_data=(X_val, y_val_cat) if X_val.shape[0] > 0 else None,\n",
    "      callbacks=callbacks_list,\n",
    "      verbose=1\n",
    "    )\n",
    "    print(\"--- Model Training Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# prompt: show the graph between training accuracy and testing accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "if X_train.shape[0] > 0 and history is not None:\n",
    "    # Plot training and validation accuracy curves\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    if 'val_accuracy' in history.history:\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training and validation loss curves\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot plot accuracy/loss curves: Training data not available or history object is empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# prompt: I wanna know the train accuracy and also print a heatmap for all the classes as well\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Evaluate the model on the training data\n",
    "# Note: It's often better to evaluate on a separate test set to gauge generalization\n",
    "# If you specifically need train accuracy, calculate it here:\n",
    "if 'model' in locals() and model is not None and X_train.shape[0] > 0:\n",
    "    # Unpack the model object from the tuple\n",
    "    model_obj, _ = model\n",
    "    train_predictions = model_obj.predict(X_train)\n",
    "    train_predicted_classes = np.argmax(train_predictions, axis=1)\n",
    "    train_true_classes = np.argmax(y_train_cat, axis=1) # Use y_train_cat for comparison\n",
    "\n",
    "    train_accuracy = accuracy_score(train_true_classes, train_predicted_classes)\n",
    "    print(f\"\\nTraining Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    # Generate Confusion Matrix for training data\n",
    "    cm_train = confusion_matrix(train_true_classes, train_predicted_classes)\n",
    "\n",
    "    # Plotting the heatmap for training confusion matrix\n",
    "    plt.figure(figsize=(15, 12)) # Adjusted figure size for potentially many classes\n",
    "    sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', xticklabels=CLASSES, yticklabels=CLASSES)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.title('Confusion Matrix (Training Data)')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Cannot calculate training accuracy or plot heatmap: Model not available or training data is empty.\")\n",
    "\n",
    "# Evaluate on Test Data and print heatmap for all classes\n",
    "if 'model' in locals() and model is not None and X_test.shape[0] > 0:\n",
    "    print(\"\\n--- Evaluating on Test Data ---\")\n",
    "    # Unpack the model object from the tuple\n",
    "    model_obj, _ = model\n",
    "    loss, accuracy = model_obj.evaluate(X_test, y_test_categorical, verbose=0) # Use y_test_categorical\n",
    "    print(f\"Test Loss: {loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Generate Confusion Matrix for test data\n",
    "    test_predictions = model_obj.predict(X_test)\n",
    "    test_predicted_classes = np.argmax(test_predictions, axis=1)\n",
    "    test_true_classes = np.argmax(y_test_categorical, axis=1) # Use y_test_categorical\n",
    "\n",
    "    cm_test = confusion_matrix(test_true_classes, test_predicted_classes)\n",
    "\n",
    "    # Plotting the heatmap for test confusion matrix\n",
    "    plt.figure(figsize=(15, 12)) # Adjusted figure size for potentially many classes\n",
    "    sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', xticklabels=CLASSES, yticklabels=CLASSES)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.title('Confusion Matrix (Test Data)')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Cannot evaluate on test data or plot heatmap: Model not available or test data is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
